import os
import re
import io
import logging
import unicodedata
import time
import hashlib
from datetime import datetime, timezone
from typing import List, Tuple, Dict, Optional

# Bibliotecas externas
from dotenv import load_dotenv
from pymongo import MongoClient
from pymongo.operations import SearchIndexModel
from docx import Document
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload

# M√≥dulos locais
from lib.gemini_embendding import gerarEmbedding

# ============================================================================
# 1. CONFIGURA√á√ïES E LOGGING
# ============================================================================
load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("sync_ms_inteligente.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Configura√ß√µes do Ambiente (carregadas do arquivo .env)
ID_PASTA_DRIVE = os.getenv("ID_PASTA_DRIVE")
FILE_CREDENTIALS = os.getenv("FILE_CREDENTIALS", "credentials.json")
URI_MONGO = os.getenv("MONGODB_URI")

if not ID_PASTA_DRIVE:
    raise ValueError("‚ùå ID_PASTA_DRIVE n√£o definido! Configure no arquivo .env")
if not URI_MONGO:
    raise ValueError("‚ùå MONGODB_URI n√£o definido! Configure no arquivo .env")
DB_NAME = "ministerio_saude"
COL_DADOS = "faq_medicamentos"
COL_META = "sync_metadata" 

# ============================================================================
# 2. FERRAMENTAS DE TRATAMENTO DE TEXTO
# ============================================================================

def normalizar_para_busca(texto: str) -> str:
    """Padroniza o texto para que o chatbot encontre respostas sem erro de acento."""
    if not texto: return ""
    nksel = unicodedata.normalize('NFKD', texto)
    sem_acentos = "".join([c for c in nksel if not unicodedata.combining(c)])
    limpo = re.sub(r'[^\w\s]', '', sem_acentos)
    return re.sub(r'\s+', ' ', limpo).strip().lower()

def converter_para_markdown(p) -> str:
    """Preserva a formata√ß√£o de listas do Word para o Chatbot."""
    texto = p.text.strip()
    if p.style.name.startswith('List') or texto.startswith(('‚Ä¢', '-', '*', '‚û¢')):
        texto_limpo = re.sub(r'^[‚Ä¢\-*‚û¢]\s*', '', texto)
        return f"- {texto_limpo}"
    return texto

def extrair_tags_e_fonte(paragrafos: List[str], i: int) -> Tuple[List[str], str]:
    """Busca metadados ao redor da pergunta/resposta encontrada."""
    janela = " ".join(paragrafos[max(0, i-1):min(len(paragrafos), i+2)])
    
    f_match = re.search(r'(?:FONTE:|Ref:|\(Ref:)\s*([^)\n\t]+)', janela, re.IGNORECASE)
    fonte = f_match.group(1).strip().rstrip('.)') if f_match else ""

    t_match = re.search(r'TAGS:\s*(.+?)(?=\s*P:|\s*PERGUNTA:|$|\n)', janela, re.IGNORECASE)
    tags = [t.strip().lower() for t in re.split(r'[,\s]+', t_match.group(1).replace('#', '')) if t.strip()] if t_match else []
    
    return tags, fonte

def gerar_hash_conteudo(pergunta: str, resposta: str) -> str:
    """Gera hash MD5 do conte√∫do para detectar mudan√ßas."""
    conteudo = f"{pergunta}|{resposta}"
    return hashlib.md5(conteudo.encode('utf-8')).hexdigest()

def carregar_embeddings_existentes(collection, file_id: str) -> Dict[str, List[float]]:
    """Carrega embeddings existentes indexados por hash do conte√∫do."""
    cache = {}
    docs = collection.find(
        {"file_id": file_id, "content_hash": {"$exists": True}, "embedding": {"$ne": None}},
        {"content_hash": 1, "embedding": 1}
    )
    for doc in docs:
        cache[doc["content_hash"]] = doc["embedding"]
    return cache

# ============================================================================
# 3. CONFIGURA√á√ÉO DO √çNDICE VETORIAL ATLAS
# ============================================================================

def criar_indice_vetorial(collection):
    """Cria o √≠ndice vetorial no MongoDB Atlas para busca sem√¢ntica."""
    # Dimens√£o do embedding do Gemini gemini-embedding-001 √© 768
    EMBEDDING_DIMENSION = 768
    INDEX_NAME = "vector_index"
    
    # Verifica se o √≠ndice j√° existe
    existing_indexes = list(collection.list_search_indexes())
    if any(idx.get("name") == INDEX_NAME for idx in existing_indexes):
        logger.info(f"‚úÖ √çndice vetorial '{INDEX_NAME}' j√° existe.")
        return
    
    search_index_model = SearchIndexModel(
        definition={
            "fields": [
                {
                    "type": "vector",
                    "path": "embedding",
                    "numDimensions": EMBEDDING_DIMENSION,
                    "similarity": "cosine"
                },
                {
                    "type": "filter",
                    "path": "isActive"
                },
                {
                    "type": "filter",
                    "path": "category"
                }
            ]
        },
        name=INDEX_NAME,
        type="vectorSearch"
    )
    
    try:
        collection.create_search_index(model=search_index_model)
        logger.info(f"‚úÖ √çndice vetorial '{INDEX_NAME}' criado com sucesso.")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel criar √≠ndice vetorial: {e}")

# ============================================================================
# 4. L√ìGICA DE SINCRONIZA√á√ÉO INTELIGENTE
# ============================================================================

def processar_faqs_drive(db) -> Tuple[int, int]:
    col_dados = db[COL_DADOS]
    col_meta = db[COL_META]
    
    creds = service_account.Credentials.from_service_account_file(
        FILE_CREDENTIALS, scopes=['https://www.googleapis.com/auth/drive.readonly'])
    service = build('drive', 'v3', credentials=creds)

    query = f"'{ID_PASTA_DRIVE}' in parents and name contains '.docx' and mimeType = 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'"
    results = service.files().list(q=query, fields="files(id, name, modifiedTime)").execute()
    arquivos = results.get('files', [])

    itens_novos_total = 0
    arquivos_pulados = 0
    LIMITE_PERGUNTAS = 100  # Limite de perguntas para esta execu√ß√£o

    for arq in arquivos:
        # Verifica se j√° atingiu o limite de perguntas
        if itens_novos_total >= LIMITE_PERGUNTAS:
            logger.info(f"üõë Limite de {LIMITE_PERGUNTAS} perguntas atingido. Parando processamento.")
            break
        file_id = arq['id']
        nome_arq = arq['name']
        data_drive = arq['modifiedTime']

        meta = col_meta.find_one({"file_id": file_id})
        if meta and meta.get('last_modified') == data_drive:
            arquivos_pulados += 1
            continue

        logger.info(f"üîÑ Atualizando: {nome_arq}")
        try:
            # Carregar cache de embeddings existentes antes de deletar
            cache_embeddings = carregar_embeddings_existentes(col_dados, file_id)
            embeddings_reutilizados = 0
            embeddings_gerados = 0
            
            request = service.files().get_media(fileId=file_id)
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            done = False
            while not done: _, done = downloader.next_chunk()
            
            doc = Document(fh)
            linhas = [converter_para_markdown(p) for p in doc.paragraphs if p.text.strip()]
            
            lote_arquivo = []
            categoria_atual = nome_arq.replace("FAQ", "").replace(".docx", "").strip().lower()
            
            # Vari√°veis de controle para rastreamento de perguntas multi-linha
            pergunta_pendente = ""
            linha_inicio_pergunta = 0
            perguntas_no_arquivo = 0  # Contador para este arquivo

            # Itera√ß√£o sobre os par√°grafos do documento
            for i, linha in enumerate(linhas):
                try:
                    num_linha_real = i + 1 # Para facilitar a localiza√ß√£o manual no Word (que n√£o come√ßa em 0)

                    # 1. Verifica√ß√£o de troca de Assunto/Categoria
                    assunto_m = re.search(r'\[ASSUNTO:\s*(.+?)\]', linha, re.IGNORECASE)
                    if assunto_m: 
                        categoria_atual = assunto_m.group(1).strip().lower()
                        continue

                    pergunta, resposta = None, None

                    # 2. EXTRA√á√ÉO: Caso Pergunta e Resposta estejam na MESMA LINHA
                    if re.search(r'\b(P|PERGUNTA):\s*', linha, re.IGNORECASE) and re.search(r'\b(R|RESPOSTA):\s*', linha, re.IGNORECASE):
                        partes = re.split(r'\s*\b(R|RESPOSTA):\s*', linha, flags=re.IGNORECASE)
                        pergunta = re.sub(r'(\d+\.\s*)?\b(P|PERGUNTA):\s*', '', partes[0], flags=re.IGNORECASE).strip()
                        # Extrai a resposta removendo poss√≠veis tags/fontes que vierem na mesma linha
                        resposta = re.split(r'tags:|fonte:|ref:|\(ref:', partes[2], flags=re.IGNORECASE)[0].strip()

                    # 3. EXTRA√á√ÉO: Caso seja apenas o in√≠cio de uma PERGUNTA (P:)
                    elif re.search(r'^(\d+\.\s*)?\b(P|PERGUNTA):\s*', linha, re.IGNORECASE):
                        pergunta_pendente = re.sub(r'^(\d+\.\s*)?\b(P|PERGUNTA):\s*', '', linha, flags=re.IGNORECASE).strip()
                        linha_inicio_pergunta = num_linha_real
                        continue

                    # 4. EXTRA√á√ÉO: Caso seja a RESPOSTA (R:) para uma pergunta detectada anteriormente
                    elif re.search(r'^\b(R|RESPOSTA):\s*', linha, re.IGNORECASE):
                        if pergunta_pendente:
                            pergunta = pergunta_pendente
                            # Limpa o prefixo 'R:' e remove metadados do final
                            corpo_res = re.sub(r'^\b(R|RESPOSTA):\s*', '', linha, flags=re.IGNORECASE)
                            resposta = re.split(r'tags:|fonte:|ref:|\(ref:', corpo_res, flags=re.IGNORECASE)[0].strip()
                            pergunta_pendente = "" # Reseta para a pr√≥xima captura
                        else:
                            # Log de aviso: encontrou um R: mas n√£o viu o P: antes
                            logger.warning(f"  ‚ö†Ô∏è Resposta sem pergunta correspondente na linha {num_linha_real} de '{nome_arq}'")

                    # Se conseguimos formar um par P&R, salvamos no lote
                    if pergunta and resposta:
                        # Verificar limite ANTES de processar
                        total_ate_agora = itens_novos_total + perguntas_no_arquivo
                        if total_ate_agora >= LIMITE_PERGUNTAS:
                            logger.info(f"üõë Limite de {LIMITE_PERGUNTAS} perguntas atingido. Parando processamento do arquivo.")
                            break
                        
                        perguntas_no_arquivo += 1
                        tags, fonte = extrair_tags_e_fonte(linhas, i)
                        
                        # Verificar se j√° temos embedding cacheado para este conte√∫do
                        content_hash = gerar_hash_conteudo(pergunta, resposta)
                        embedding_vector = cache_embeddings.get(content_hash)
                        
                        if embedding_vector:
                            embeddings_reutilizados += 1
                            logger.info(f"   üìå [{total_ate_agora + 1}/{LIMITE_PERGUNTAS}] Reutilizando embedding...")
                        else:
                            # Gerar novo embedding apenas se o conte√∫do mudou
                            texto_para_embedding = f"{pergunta} {resposta}"
                            try:
                                logger.info(f"   üîÑ [{total_ate_agora + 1}/{LIMITE_PERGUNTAS}] Gerando embedding...")
                                embedding_result = gerarEmbedding(texto_para_embedding)
                                embedding_vector = embedding_result.embeddings[0].values
                                embeddings_gerados += 1
                            except Exception as emb_error:
                                logger.warning(f"  ‚ö†Ô∏è Falha ao gerar embedding na linha {num_linha_real}: {emb_error}")
                                embedding_vector = None
                        
                        lote_arquivo.append({
                            "question": pergunta,
                            "question_normalized": normalizar_para_busca(pergunta),
                            "answer": resposta,
                            "category": categoria_atual,
                            "tags": tags,
                            "source": fonte,
                            "file_id": file_id,
                            "file_origin": nome_arq,
                            "line_reference": num_linha_real,
                            "content_hash": content_hash,  # Hash para cache de embeddings
                            "isActive": True,
                            "updatedAt": datetime.now(timezone.utc),
                            "embedding": embedding_vector
                        })

                except Exception as line_error:
                    # RASTREABILIDADE: Loga o erro sem parar o processamento do resto do arquivo
                    logger.error(f"  ‚ùå Erro ao processar par√°grafo na linha {i+1} do arquivo '{nome_arq}': {line_error}")
                    continue

            # Verifica√ß√£o de seguran√ßa: sobrou pergunta sem resposta no final do arquivo?
            if pergunta_pendente:
                logger.warning(f"  ‚ö†Ô∏è Pergunta detectada na linha {linha_inicio_pergunta} de '{nome_arq}' ficou sem resposta (R:).")

            # ATUALIZA√á√ÉO AT√îMICA POR ARQUIVO
            if lote_arquivo:
                col_dados.delete_many({"file_id": file_id})
                col_dados.insert_many(lote_arquivo)
                col_meta.update_one(
                    {"file_id": file_id},
                    {"$set": {"last_modified": data_drive, "updated_at": datetime.now(timezone.utc)}},
                    upsert=True
                )
                itens_novos_total += len(lote_arquivo)
                logger.info(f"   ‚úîÔ∏è Sucesso: {len(lote_arquivo)} itens sincronizados.")
                logger.info(f"   üí∞ Embeddings: {embeddings_reutilizados} reutilizados, {embeddings_gerados} novos gerados.")

        except Exception as file_error:
            logger.error(f"   ‚ùå Falha cr√≠tica ao processar o arquivo {nome_arq}: {file_error}")

    return itens_novos_total, arquivos_pulados

# ============================================================================
# 5. EXECU√á√ÉO
# ============================================================================

def main():
    tempo_start = time.time()
    client = MongoClient(URI_MONGO)
    
    try:
        db = client[DB_NAME]
        col_dados = db[COL_DADOS]
        
        print("\n" + "‚ïê"*60)
        logger.info("üöÄ INICIANDO SINCRONIZADOR INTELIGENTE (MODO INCREMENTAL)")
        
        # Garante que o √≠ndice vetorial existe
        criar_indice_vetorial(col_dados)
        
        novos, pulados = processar_faqs_drive(db)
        
        total_ativos = col_dados.count_documents({"isActive": True})

        print("\n" + "üìä RELAT√ìRIO FINAL DE OPERA√á√ÉO")
        print("‚îÄ"*60)
        print(f"‚è≠Ô∏è  Arquivos Pulados (Sem altera√ß√£o): {pulados}")
        print(f"üì• Itens Novos/Atualizados:         {novos}")
        print(f"üü¢ Total de FAQ Ativas no Chatbot:  {total_ativos}")
        print(f"üïí Tempo de execu√ß√£o:               {time.time() - tempo_start:.2f}s")
        print("‚ïê"*60 + "\n")

    except Exception as e:
        logger.critical(f"Falha Cr√≠tica na execu√ß√£o principal: {e}")
    finally:
        client.close()

if __name__ == "__main__":
    main()